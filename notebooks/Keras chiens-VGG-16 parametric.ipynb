{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image labelling using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook does a transfer learning based on the VGG-16 pre-trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(123)  # for reproducibility\n",
    "\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import scipy.io\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras import Model\n",
    "\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting metadata out of Matlab file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to read picture information from a matlab file\n",
    "def load_img_from_mat(mat_file):\n",
    "\n",
    "    mat_data = scipy.io.loadmat(mat_file)\n",
    "    # load annotations for each picture\n",
    "    dogs_annotation = []\n",
    "    \n",
    "    for img_annotation in mat_data['annotation_list']:\n",
    "        dog_annotation = str(img_annotation[0][0])\n",
    "        dogs_annotation.append(dog_annotation)\n",
    "\n",
    "    # load picture filenames and path for each picture contained in mat file\n",
    "    dogs_full_path = []\n",
    "    dogs_folder = []\n",
    "    dogs_file = []\n",
    "\n",
    "    for img_file in mat_data['file_list']:\n",
    "        dog_full_path = str(img_file[0][0])\n",
    "        folder, filen =  dog_full_path.split('/')\n",
    "        dogs_full_path.append(dog_full_path)\n",
    "        dogs_folder.append(folder)\n",
    "        dogs_file.append(filen)\n",
    "\n",
    "    # Create dataframe with these informations\n",
    "    file_df = pd.DataFrame({'file' : dogs_file,\n",
    "                             'folder' : dogs_folder,\n",
    "                             'full_path' : dogs_full_path,\n",
    "                             'annotation' : dogs_annotation})\n",
    "    \n",
    "    # Load label information and create a complete dataframe\n",
    "    labels_df = pd.DataFrame(mat_data['labels'], columns = ['label'])\n",
    "    file_df = file_df.join(labels_df)\n",
    "    \n",
    "    return file_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading pictures and storing them into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to read a file picture and return it as VGG-16 requires\n",
    "def load_img_file(variant, img_folder, img_file):\n",
    "    picture_file = join(variant, img_folder, img_file)\n",
    "\n",
    "    # Load picture and resez all to 224x224 pixels\n",
    "    img = load_img(picture_file, target_size=(224, 224))\n",
    "    \n",
    "    # Picture has to be converted into Numpy array\n",
    "    img = img_to_array(img)\n",
    "    \n",
    "    # Preprocess input as VGG-16 requires it\n",
    "    img = preprocess_input(img)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create picture array\n",
    "def get_pict_array(pict_df):\n",
    "    # Variable to hold all pictures\n",
    "    X_pict_array = []\n",
    "    first = True\n",
    "    for dog_folder, dog_file in np.array(pict_df[['folder', 'file']]):\n",
    "        img = load_img_file(images, dog_folder, dog_file)\n",
    "        if first:\n",
    "            X_pict_array = [img]\n",
    "            first = False\n",
    "        else:\n",
    "            X_pict_array += [img]\n",
    "            \n",
    "    return np.array(X_pict_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directories\n",
    "my_data = \"../data\"\n",
    "images = my_data + '/Images' # Images folder\n",
    "bw = my_data + '/BWEqu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Number of dogs breeds to include\n",
    "nb_breeds = 10\n",
    "\n",
    "# Fine tuning strategy (values: full, partial, feature)\n",
    "strategy = 'partial'\n",
    "\n",
    "# number of epochs for fine tuning\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "train_df = load_img_from_mat(my_data + '/train_list.mat')\n",
    "sample_train_df = train_df[train_df['label'] <= nb_breeds]\n",
    "\n",
    "X_train = get_pict_array(sample_train_df)\n",
    "Y_train = sample_train_df['label'] - 1\n",
    "\n",
    "# Load test data\n",
    "test_df = load_img_from_mat(my_data + '/test_list.mat')\n",
    "sample_test_df = test_df[test_df['label'] <= nb_breeds]\n",
    "\n",
    "X_test = get_pict_array(sample_test_df)\n",
    "Y_test = sample_test_df['label'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# Checking imported data. shape returns \n",
    "# - the number of pictures\n",
    "# - The byte depth: 3 = color picture\n",
    "# - the size in pixels\n",
    "\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all values to be between 0 and 1\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess class labels to create a matrix\n",
    "Y_train = np_utils.to_categorical(Y_train, nb_breeds)\n",
    "Y_test = np_utils.to_categorical(Y_test, nb_breeds)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the VGG-16 network pre-trained with Imagenet. Remove upper layer.\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(3, 224, 224))\n",
    "\n",
    "# get the ouput of the model to build on top of it\n",
    "x = base_model.output\n",
    "\n",
    "# Add a Flatten layer to adapt the data format to the Dense layer requirements\n",
    "x = Flatten()(x)\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "\n",
    "# Add a new fully connected layer with numer of classes depending on the number of labels\n",
    "predictions = Dense(nb_breeds, activation='softmax')(x)\n",
    "\n",
    "# Finally define the new model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stategy choice : feature extraction only, partial fine tuning or complete fine tuning\n",
    "# Here : complete fine tuning, all layers are trainanble\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "if strategy == 'full':\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "if strategy == 'partial':\n",
    "    for layer in base_model.layers[6:]:\n",
    "        layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=0.0001, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 224, 224)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 224, 224)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 112, 112)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 112, 112)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 112, 112)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 128, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 256, 56, 56)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 256, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 512, 28, 28)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 512, 7, 7)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 134,301,514\n",
      "Trainable params: 134,041,354\n",
      "Non-trainable params: 260,160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 512, 7, 7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[19].input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 444s 444ms/step - loss: 2.3352 - acc: 0.1230\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 441s 441ms/step - loss: 2.0297 - acc: 0.3390\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 395s 395ms/step - loss: 1.6354 - acc: 0.5310\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 396s 396ms/step - loss: 1.1214 - acc: 0.6780\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 384s 384ms/step - loss: 0.7044 - acc: 0.7950\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 383s 383ms/step - loss: 0.3968 - acc: 0.8960\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 386s 386ms/step - loss: 0.2168 - acc: 0.9530\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 442s 442ms/step - loss: 0.1076 - acc: 0.9860\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 476s 476ms/step - loss: 0.0452 - acc: 0.9990\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 509s 509ms/step - loss: 0.0208 - acc: 1.0000\n",
      "Fitting time 4261.647929430008 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Fit model on training data\n",
    "start = time.time()\n",
    "\n",
    "model.fit(X_train, Y_train, \n",
    "          batch_size=32, epochs=epochs, verbose=1)\n",
    "end = time.time()\n",
    "print('Fitting time {0} seconds.'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for 10 breeds, partial strategy and 10 epochs: 69.9%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score for {0} breeds, {1} strategy and {2} epochs: {3:.1%}\".format(nb_breeds, strategy, epochs, score[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping model vgg16-10-partial-10.pkl\n"
     ]
    }
   ],
   "source": [
    "#Pickle objects\n",
    "import pickle\n",
    "\n",
    "#dump the model\n",
    "model_name = 'vgg16-{0}-{1}-{2}.pkl'.format(nb_breeds, strategy, epochs)\n",
    "print('Dumping model {0}'.format(model_name))\n",
    "output = open(model_name, 'wb')\n",
    "pickle.dump(model, output, -1)\n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
